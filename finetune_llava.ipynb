{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 15:38:46.430270: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-06 15:38:46.443824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-06 15:38:46.461410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-06 15:38:46.466597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-06 15:38:46.479670: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 15:38:47.923493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import re\n",
    "import bisect\n",
    "import shutil\n",
    "import numpy as np\n",
    "from nltk import edit_distance\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "from transformers import BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "MODEL_ID = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "REPO_ID = \"RaushanTurganbay/VideoLLava-demo\" # Change to your hf-hub repo\n",
    "\n",
    "USE_LORA = True\n",
    "USE_QLORA = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(video_path, start, end):\n",
    "    \"\"\"Reads a video for given start-end timestamps interval and uniformly samples 8 frames of it\"\"\"\n",
    "    container = av.open(video_path)\n",
    "    video = container.streams.get(0)[0]\n",
    "\n",
    "    av_timestamps = [\n",
    "        int(packet.pts * video.time_base) for packet in container.demux(video) if packet.pts is not None\n",
    "    ]\n",
    "\n",
    "    av_timestamps.sort()\n",
    "    start_id = bisect.bisect_left(av_timestamps, start)\n",
    "    end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "\n",
    "    if end_id - start_id < 10:\n",
    "        end_id = min(len(av_timestamps) - 1, end_id + 10)\n",
    "        start_id = max(0, start_id - 10)\n",
    "\n",
    "\n",
    "    end_id = min(len(av_timestamps) - 1, end_id)\n",
    "    start_id = max(0, start_id)\n",
    "\n",
    "    num_frames_to_sample = min(2, end_id - start_id + 1)\n",
    "    indices = np.linspace(start_id, end_id, num_frames_to_sample).astype(int)\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_id:\n",
    "            break\n",
    "        if i >= start_id and i in indices:\n",
    "            frames.append(frame)\n",
    "    assert len(frames) == 2, f\"Got {len(frames)} frames but should be 2. Check the indices: {indices};, start_id: {start_id}, end_id: {end_id}. Len of video is {len(av_timestamps)} frames.\"\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_read_video(example, path):\n",
    "    # Some datasets have a start-end interval, so we try to get it if exists. Otherwise just set a very large end timestamp\n",
    "    clip = read_video_pyav(f'{path}/{example[\"video\"]}', example.get(\"start\", 1), example.get(\"end\", 1e+10))\n",
    "    example[\"clip\"] = clip\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VideoLlavaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for VideoLlavaDataset. This class takes a HuggingFace Dataset as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, video_path):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.video_path = video_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.dataset[idx]\n",
    "        # Lazy load video clip here\n",
    "        clip = read_video_pyav(f'{self.video_path}/{sample[\"video\"]}', sample.get(\"start\", 0), sample.get(\"end\", 1e+10))\n",
    "        answer = sample['conversations'][1]['value']\n",
    "        tmp_prompt = sample['conversations'][0]['value']\n",
    "\n",
    "        prompt = f\"USER: {tmp_prompt}\" \\\n",
    "                 f\"\\n ASSISTANT: Answer: {answer}\"\n",
    "\n",
    "        return prompt, clip, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    videos = []\n",
    "    texts = []\n",
    "    texts, videos, _ = list(zip(*examples))\n",
    "\n",
    "    batch = processor(text=texts, videos=videos, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # We don't want to compute loss for pad tokens, lets mask with -100. Some methods also mask the prompt, calculating loss only on the answers/captions/etc\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values_videos = batch[\"pixel_values_videos\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values_videos, labels\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    # We only feed the prompt to the model\n",
    "    # Make sure to separate prompt from answers/captions/etc depending on your own task and dataset\n",
    "    # Otherwise your model will peek into the ground truth\n",
    "    videos = []\n",
    "    texts = []\n",
    "    true_answers = []\n",
    "    texts, videos, true_answers = list(zip(*examples))\n",
    "    texts = [text[:-2] for text in texts]  # Get text without answers, so the model has to generate the answers itself during eval\n",
    "\n",
    "    batch = processor(text=texts, videos=videos, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values_videos = batch[\"pixel_values_videos\"]\n",
    "    # answer_choice = batch[\"answer\"] \n",
    "\n",
    "    return input_ids, attention_mask, pixel_values_videos, true_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('VLM/sample_annotations.json', 'r') as file:\n",
    "    train_data  = json.load(file)\n",
    "\n",
    "with open('VLM/sample_annotations.json', 'r') as file:\n",
    "    test_data  = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for training data\n",
    "train_dataset_dict = {\n",
    "    \"id\": [item['id'] for item in train_data],\n",
    "    \"video\": [item['video'] for item in train_data],\n",
    "    \"conversations\": [item['conversations'] for item in train_data],\n",
    "}\n",
    "\n",
    "# Create dictionary for testing data\n",
    "test_dataset_dict = {\n",
    "    \"id\": [item['id'] for item in test_data],\n",
    "    \"video\": [item['video'] for item in test_data],\n",
    "    \"conversations\": [item['conversations'] for item in test_data],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert these dictionaries to HuggingFace datasets\n",
    "train_dataset_tmp = Dataset.from_dict(train_dataset_dict)\n",
    "test_dataset_tmp = Dataset.from_dict(test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " 'video': ['airport_hangar_5_2-airport_hangar_5_4.mp4',\n",
       "  'airport_hangar_5_4-airport_hangar_5_5.mp4',\n",
       "  'airport_hangar_7_9-airport_hangar_7_5.mp4',\n",
       "  'airport_hangar_10_5-airport_hangar_10_3.mp4',\n",
       "  'airport_hangar_10_2-airport_hangar_10_0.mp4',\n",
       "  'airport_hangar_12_0-airport_hangar_12_1.mp4',\n",
       "  'airport_hangar_13_4-airport_hangar_13_5.mp4',\n",
       "  'airport_hangar_14_5-airport_hangar_14_9.mp4',\n",
       "  'airport_hangar_14_0-airport_hangar_14_7.mp4',\n",
       "  'airport_hangar_16_1-airport_hangar_16_2.mp4'],\n",
       " 'conversations': [[{'from': 'human',\n",
       "    'value': '<video>\\n Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '**Image 1:**  \\nThis aerial view showcases a large paved area, likely an airport runway, with several small aircraft parked nearby. Surrounding the runway and aircraft are various buildings, some of which appear to be hangars or service facilities, along with a parking lot filled with vehicles.\\n\\n**Image 2:**  \\nThe second image presents a similar aerial perspective but reveals more of the surrounding infrastructure. The runway remains prominent, with many small airplanes parked along its length. The layout of buildings is more defined, and a noticeable increase in greenery is observable in the surroundings. Additionally, there are more vehicles parked and some residential areas visible just beyond the airport premises.\\n\\n### Changes Observed Between the Images\\n\\nThe key changes between the two images include:\\n\\n1. **Perspective and Detail:** The second image provides a clearer and more detailed view, likely taken from a closer distance or at a higher resolution.\\n   \\n2. **Increased Infrastructure:** There are additional vehicles and structures visible in the second image, indicating possible expansion or development around the airport area.\\n\\n3. **Greenery and Landscaping:** The presence of more trees and shrubs in the second image suggests landscaping enhancements that may have been implemented since the first image was captured.\\n\\n4. **Aircraft Positioning:** While both images show small aircraft, the second image depicts a different arrangement or greater number, hinting at increased activity or usage at the airport.\\n\\nThese changes reflect the evolution of the airport environment, including development, beautification, and possibly increased operational capacity.'}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': \"1. **First Image**: This aerial view shows a small airport with multiple aircraft parked on the tarmac. A wide runway is visible in the upper portion, with several white planes arranged neatly. At the bottom, there's a mix of buildings and residential areas, with a clear view of the parking lots and roadways surrounding the airport facility.\\n\\n2. **Second Image**: This image also depicts the same airport layout, but with less clarity. The tarmac is less visible, and the overall scene appears darker, possibly indicating an overcast sky or shade. The arrangement of planes remains similar to the first image, but some details could be less defined due to the lighting or weather changes.\\n\\n### Changes Between Images\\n\\nThe primary change between the two images is the visibility and lighting. The first image is brightly lit, allowing for clear views of all elements, while the second image has a darker tone, suggesting different weather conditions or time of day. Despite the changes in visibility, the layout of the airport and the positioning of the aircraft remain consistent, indicating that the observations are from similar perspectives but under varying lighting circumstances.\"}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '**Image 1:**\\nThe first image depicts an aerial view of an industrial area, likely a cargo facility at an airport. Key features include a large, white-roofed warehouse structure, multiple vehicles parked in organized lots, and tanks visible on the ground. An airplane is seen on the runway, suggesting connectivity to air transport.\\n\\n**Image 2:**\\nThe second image presents a nearly identical view but appears darker or shadowed. The large white warehouse remains prominent, but the overall lighting leads to a more subdued appearance. The distribution of vehicles and tanks looks similar, but there may be some obscuring shadows affecting visibility.\\n\\n### Changes Between the Images\\n\\nThe main differences between the two images are related to lighting and visibility. While both images depict the same area, the second image has a darker tone, possibly due to cloud cover or changes in lighting conditions. The overall layout of structures, vehicles, and tanks appears consistent, indicating no significant physical changes within the observed timeframe. The differences are primarily aesthetic rather than structural.'}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': \"**Image 1:**\\nThe first image shows a satellite view of a mixed landscape featuring a dense forest area on the top and left sides. In the lower section, there is a row of buildings along a road, with a clear view of some infrastructure, including vehicles. A prominent structure is visible at the center-right, possibly a warehouse or industrial building, surrounded by greenery. \\n\\n**Image 2:**\\nThe second image is similar but shows changes that indicate some alterations in the landscape. The same dense forest area and the row of buildings can be seen. However, the central building has shifted slightly, and changes in the surrounding vegetation are more noticeable, suggesting some areas may have been cleared or undergone development.\\n\\n### Changes Between Images\\nThe primary changes between the two images involve shifts in the central building's position and the surrounding vegetation. In the second image, the building appears to have experienced some lateral movement or alteration in orientation, while additional vegetation removal is evident, leading to a clearer view of the infrastructure and nearby area. These changes could indicate ongoing development or land management activities between the two captures.\"}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Give a detailed account of the satellite video, with each frame depicting the same location at distinct points in time.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '**Image 1:**\\nThis aerial view shows a landscape primarily consisting of dense green forest. In the middle-right area, there is a rectangular building, likely a warehouse or storage facility, with some surrounding cleared land. A dirt path runs through the forest, and a busy road is visible at the bottom with several vehicles and buildings lining it.\\n\\n**Image 2:**\\nThis image features a similar aerial view of the same area. The forest remains dense, but noticeable changes are evident. The rectangular building is still present, but the cleared area around it has expanded, revealing more open ground. The dirt path looks more defined, and the road at the bottom is busy as well, similar to the first image, but with slight changes in the vehicle arrangement.\\n\\n### Changes Between the Images:\\n\\n1. **Cleared Area Expansion:** The space surrounding the building has increased in size, indicating possible construction or land clearing activities between the two images.\\n2. **Path Definition:** The dirt path appears more prominent in the second image, suggesting enhancements or maintenance to the route.\\n3. **Vegetation Density:** The forested area appears slightly denser in the first image, while the second might indicate managed growth or slight changes in vegetation.\\n4. **Road Changes:** While the road appears mostly consistent, there could be minor differences in the arrangement of vehicles or nearby structures, although these changes are less pronounced compared to the building and cleared area. \\n\\nOverall, the series of images indicates a gradual alteration of the landscape, likely due to human activity or land management practices.'}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': \"1. **First Image:**\\n   - The image shows an aerial view of a facility surrounded by greenery. Prominently, there is a large buildingâ€”possibly a hangarâ€”depicted in a dark color. Several smaller buildings are visible nearby, along with an arrangement of aircraft on the ground, suggesting that this might be an airfield or aviation-related location. There are also paved paths and areas with trees, with indications of additional structures and possibly recreational areas.\\n\\n2. **Second Image:**\\n   - The second image also presents an aerial view of the same facility, but it is rendered in a darker, more subdued tone. The layout of the hangar and buildings remains consistent with the first image, but the visibility of certain features is reduced due to the overall darker palette. The arrangement of aircraft along the ground is still discernible, though the details may appear less pronounced compared to the first image.\\n\\n### Changes Between the Images:\\n\\n- The primary change between the two images is the color and brightness. The first image features a lighter, more vibrant color scheme, making it easier to distinguish details, while the second image exhibits a darker tone that may obscure some features. \\n- Despite these tonal differences, the spatial arrangement of buildings and aircraft remains consistent, indicating that the two images likely depict the same location at different times or under different lighting or imaging conditions. The changes in contrast may affect the viewer's ability to perceive smaller details but do not indicate any structural alterations in the landscape itself.\"}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Describe the satellite video thoroughly, noting that each frame shows the same location at a different time.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': \"**Image 1:**\\nThis aerial image captures a busy urban area featuring a mix of commercial buildings, roads, and an airport section with multiple small aircraft. The layout shows dark pavements, white-topped buildings, and an expanse of cleared land on the right, possibly used as an airstrip or for future development. There are numerous parked cars in various lots, and the grid-like street pattern is evident.\\n\\n**Image 2:**\\nThe second image is similar in perspective, showing the same urban setting, but there are noticeable differences. The aircraft lining the airport area appear more clustered in a central location. Additionally, the area to the right has slight differences in texture and color, which may indicate changes in vegetation or land use. More vehicles are present on the roads, suggesting an increase in activity or traffic.\\n\\n### Changes Between the Images\\n\\nThe changes between the two images indicate a shift in the urban environment over a brief period. Notable differences include:\\n\\n1. **Aircraft Positioning:** In the second image, the placement and arrangement of aircraft appear more denser or differently organized.\\n   \\n2. **Traffic and Parking:** There's an observable increase in vehicle presence on the streets and possibly added cars in parking lots, indicating heightened activity in the area.\\n\\n3. **Land Use:** Variations in the land texture or color suggest potential landscaping changes or different uses of the land adjacent to the airport.\\n\\nThese shifts might reflect seasonal factors, urban development, or operational adjustments at the airport or surrounding businesses.\"}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '**Image 1:**\\nThis aerial view captures part of an industrial area, featuring various buildings, hangars, and parked aircraft. There are distinct circular patterns on the grassy area in the upper left, and several airplanes are visible on the tarmac. The road runs alongside a row of buildings, with a mixture of green spaces and parked vehicles.\\n\\n**Image 2:**\\nThe second image is also an aerial view of the same area, showing more activity. The circular grassy patterns remain, while additional aircraft are now parked on the tarmac, indicating a busier operational state. The layout of the buildings appears similar, but there are noticeable changes in the number of parked vehicles and the presence of more planes.\\n\\n### Changes Between Images\\n\\nThe primary changes between the two images include an increase in the number of parked aircraft and possibly a larger number of vehicles on the road. This suggests a heightened level of activity, indicating that the area may be experiencing more operations or services in the second image compared to the first. Overall, the second image reflects a more dynamic and busy environment.'}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Provide a report describing the satellite video, where each frame shows the same location at different time points.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': \"**Image 1:**  \\nThis aerial image shows an industrial area with several buildings and some open spaces. There are multiple parking lots and a road running through the middle of the scene. White vehicles, likely planes, are on the tarmac, indicating an aviation facility. The surroundings include greenery, with trees lining the road and various circular structures, possibly for holding tanks or equipment.\\n\\n**Image 2:**  \\nIn this aerial image, the same industrial area is depicted but from a slightly different angle. It still features several buildings and parking lots, but there's a notable increase in the number of vehicles on the road, suggesting busier traffic. The same planes are still visible, and the open spaces appear more organized with fewer markings now visible on the ground. The presence of more parked cars indicates a bustling environment, perhaps signaling increased activity or operations.\\n\\n### Changes Between the Images\\n\\nThe primary changes between the two images are as follows:\\n\\n1. **Traffic Density:** The second image shows more vehicles on the road. This indicates higher traffic volume, possibly reflecting increased operational activity in the area.\\n\\n2. **Ground Markings and Layout:** There appears to be fewer visible ground markings in the second image compared to the first, which could suggest recent changes or improvements to the infrastructure.\\n\\n3. **Overall Activity Level:** The increase in parked cars in the second image suggests a busier environment, possibly indicative of heightened operational activity or a change in usage patterns for the facilities shown.\\n\\nThese differences highlight changes in the usage and dynamics of the industrial area over time.\"}],\n",
       "  [{'from': 'human',\n",
       "    'value': '<video>\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.'},\n",
       "   {'from': 'gpt',\n",
       "    'value': '**Image 1:**\\nThe first image presents an aerial view of an airport or airfield area. It shows a mixture of buildings with green roofs, a wide taxiway, and several parked cars in a nearby parking lot. The layout reveals distinct structures and an organized arrangement of roads leading into and out of the airport facility. \\n\\n**Image 2:**\\nThe second image is also an aerial view of the same airport or airfield from a similar angle. It features the same buildings and parking areas, but there are newer details visible. Several small planes are parked on the tarmac, and additional vehicles are visible near the runway. There is a clear delineation of the taxiway lines and runway markings.\\n\\n### Explanation of Changes:\\n\\nThe primary change between the two images is the presence of aircraft on the tarmac in the second image, indicating increased activity or a busier operational state at the airfield. Additionally, there may be minor alterations in the arrangement of vehicles and slight differences in the visibility of the surrounding structures due to the angle or time of capture. Overall, the changes suggest a shift from a relatively quieter setting to a more operationally active one.'}]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoLlavaDataset(train_dataset_tmp, \"VLM/Videos\")\n",
    "eval_dataset = VideoLlavaDataset(test_dataset_tmp, \"VLM/Videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, clip, _= train_dataset[0]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_QLORA , USE_LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea7b4873d2945d78aac157c8d3d0501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30769e7d2cfe42c1bcd385061c27f10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load model\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# QLoRA: model uses 4-bit quantization, which helps in reducing memory usage while maintaining performance.\n",
    "# Standard LoRA:  model is loaded with standard LoRA adaptations.\n",
    "# Full Fine-Tuning: no memory optimization are done. In that case Flash Attention is used to speed up training, if hardware supports it.\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        # device_map={\"\": 0},\n",
    "    )\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        # device_map={\"\": 0},\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        # device_map={\"\": 0},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): VideoLlavaForConditionalGeneration(\n",
       "      (video_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(257, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (image_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(257, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): VideoLlavaMultiModalProjector(\n",
       "        (linear_1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (linear_2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "      (language_model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(32064, 4096, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaSdpaAttention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=11008, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLlavaModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "        # List to store predictions and ground truth during validation\n",
    "        self.results = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values_videos, labels = batch\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        \n",
    "        input_ids, attention_mask, pixel_values_videos, answers  = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            max_new_tokens=MAX_LENGTH,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        correct = 0\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            # cleaned_text = pred.split(\"\\n ASSISTANT: Answer:\", 1)[-1].strip() if \"\\n ASSISTANT: Answer:\" in pred else pred\n",
    "            # result_entry = {\n",
    "            #     'true': answer,\n",
    "            #     'generated': pred\n",
    "            # }\n",
    "            # self.results.append(result_entry)\n",
    "            correct += (pred.strip().lower() == answer.lower())\n",
    "        self.log(\"val_accuracy\", correct / len(answers))\n",
    "            \n",
    "        return correct\n",
    "\n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     # Save results at the end of validation\n",
    "    #     with open('validation_results.json', 'w') as f:\n",
    "    #         json.dump(self.results, f, indent=4)\n",
    "        \n",
    "    #     # Clear results for the next validation epoch\n",
    "    #     self.results.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(eval_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 5,\n",
    "        #   \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 1,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 1,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"save_strategy\":\"epoch\",\n",
    "}\n",
    "\n",
    "model_module = VideoLlavaModelPLModule(config, processor, model)\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", patience=3, verbose=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# api = HfApi()\n",
    "\n",
    "# class PushToHubCallback(Callback):\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "#         pl_module.model.push_to_hub(REPO_ID,\n",
    "#                                     commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "#     def on_train_end(self, trainer, pl_module):\n",
    "#         print(f\"Pushing model to the hub after training\")\n",
    "#         pl_module.processor.push_to_hub(REPO_ID,\n",
    "#                                     commit_message=f\"Training done\")\n",
    "#         pl_module.model.push_to_hub(REPO_ID,\n",
    "#                                     commit_message=f\"Training done\")\n",
    "\n",
    "# early_stop_callback = EarlyStopping(monitor=\"train_loss\", patience=3, verbose=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint  # Ensure correct import from lightning.pytorch\n",
    "\n",
    "# Define checkpoint callback to save only the most recent 5 checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=5,  # Keeps only the best 5 checkpoints\n",
    "    monitor=\"train_loss\",  # Monitor training loss for checkpointing\n",
    "    mode=\"min\",  # Minimize the train_loss\n",
    "    save_last=True,  # Always save the latest checkpoint\n",
    "    dirpath=\"./video_llava_demo/checkpointss\",  # Path to save the checkpoints\n",
    "    filename=\"video_llava-{epoch:02d}-{train_loss:.2f}\"  # Checkpoint file naming convention\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"./video_llava_demo\",\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    max_epochs=config.get(\"max_epochs\"),\n",
    "    accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "    gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "    precision=\"16-mixed\",\n",
    "    limit_val_batches=5,\n",
    "    num_sanity_val_steps=1,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    log_every_n_steps=1  # Set to 1 to log every batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processor and model locally\n",
    "processor.save_pretrained(\"./local_model\")\n",
    "model.save_pretrained(\"./local_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load processor and model from local directory\n",
    "processor = AutoProcessor.from_pretrained(\"./local_model\")\n",
    "\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the model from local directory\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    \"./local_model\",  # Load from local path\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# \"LanguageBind/Video-LLaVA-7B-hf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.44.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<video>\\n Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.\\n\\nThe satellite video showcases a busy city intersection with multiple lanes of traffic, including cars, trucks, and buses. The video captures the same location at different times, providing a comprehensive view of the traffic flow. The frames showcase the movement of vehicles, including cars and trucks, as they navigate through the intersection. The video also highlights the presence of pedestrians, with several individuals visible in the frames.\\n\\nThe video emphasizes the importance of traffic management and safety measures in a bustling city environment. The different times captured in the video demonstrate the dynamic nature of urban life, where traffic patterns and pedestrian movement are constantly changing. The video serves as a visual representation of the challenges faced by city planners and traffic engineers in managing the flow of vehicles and ensuring the safety of all road users.']\n",
      "['<video>\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.']\n",
      "['\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.']\n",
      "['<video>\\n Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.']\n",
      "['\\n Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.']\n",
      "['<video>\\n Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.']\n",
      "['\\n Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.']\n",
      "['<video>\\n Give a detailed account of the satellite video, with each frame depicting the same location at distinct points in time.']\n",
      "['\\n Give a detailed account of the satellite video, with each frame depicting the same location at distinct points in time.\\n\\nThe video starts with a view of a city from above, showing the buildings and roads. The camera then pans down to show a street with cars driving on it. The street is busy with traffic, and the camera captures the movement of the vehicles. The camera then pans back up to show the city from above again, providing a broader perspective of the urban landscape. The video then zooms in on a specific area, showing the details of the buildings and the layout of the streets. The camera captures the movement of people and vehicles in the area, providing a dynamic view of the city. Overall, the video provides a comprehensive view of the city, showcasing its urban landscape, traffic, and people.']\n",
      "['<video>\\n Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.']\n",
      "['\\n Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.']\n",
      "['<video>\\n Describe the satellite video thoroughly, noting that each frame shows the same location at a different time.']\n",
      "[\"\\n Describe the satellite video thoroughly, noting that each frame shows the same location at a different time.\\n\\nThe video starts with a view of a city from above, showing the buildings and roads. The camera then pans down to show the city from a street view, capturing the buildings and roads in detail. The camera then moves to a different location, providing a different perspective of the city. The video continues to show the city from various angles, providing a comprehensive view of the urban landscape. The camera captures the city's architecture, infrastructure, and the movement of people and vehicles. The video offers a unique perspective of the city, allowing viewers to appreciate the city's beauty and complexity.\"]\n",
      "['<video>\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.']\n",
      "['\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.']\n",
      "['<video>\\n Provide a report describing the satellite video, where each frame shows the same location at different time points.']\n",
      "['\\n Provide a report describing the satellite video, where each frame shows the same location at different time points.']\n",
      "['<video>\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.']\n",
      "['\\n Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "results = []\n",
    "pattern_to_remove = \"\\n ASSISTANT: Answer:\"\n",
    "batch_size = 1  # Define your batch size here\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"left\" # during training, one always uses padding on the right\n",
    "# model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Split test data into batches\n",
    "    for i in range(math.ceil(len(test_data) / batch_size)):\n",
    "        # Define the start and end index for the current batch\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(test_data))\n",
    "        \n",
    "        # Collect all texts and videos for the current batch\n",
    "        # batch_texts = [test['conversations'][0]['value'] for test in test_data[start_idx:end_idx]]\n",
    "        batch_texts = [f\"{test['conversations'][0]['value']}\" for test in test_data[start_idx:end_idx]]\n",
    "        batch_videos = [read_video_pyav(f'VLM/Videos/{test[\"video\"]}', 0, 1e+10) for test in test_data[start_idx:end_idx]]\n",
    "        \n",
    "        print(batch_texts)\n",
    "\n",
    "        # Process the entire batch at once\n",
    "        inputs = processor(text=batch_texts, videos=batch_videos, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        # print(inputs)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=False)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        print(generated_texts)\n",
    "        \n",
    "        cleaned_texts = [\n",
    "            generated_text for generated_text in generated_texts\n",
    "        ]\n",
    "        # Create results for the entire batch\n",
    "        for idx, test in enumerate(test_data[start_idx:end_idx]):\n",
    "            # print(cleaned_texts[idx])\n",
    "            true_value = test['conversations'][1]['value']\n",
    "            result_entry = {\n",
    "                'id': test['id'],\n",
    "                'video': test['video'],\n",
    "                'true': true_value,\n",
    "                'generated': cleaned_texts[idx]  # Add the cleaned text from the batch\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load the validation results\n",
    "with open('results1.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "# Extract the true and generated texts from the results\n",
    "truth = [entry['true'] for entry in results]\n",
    "predicted = [entry['generated'] for entry in results]\n",
    "\n",
    "# 1. Calculate BLEU scores\n",
    "bleu_scores = [sentence_bleu([t.split()], p.split()) for t, p in zip(truth, predicted)]\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# 2. Calculate BERTScore\n",
    "P, R, F1 = score(predicted, truth, lang=\"en\", verbose=True)\n",
    "\n",
    "# Average BERTScore metrics\n",
    "mean_precision = P.mean().item()\n",
    "mean_recall = R.mean().item()\n",
    "mean_f1 = F1.mean().item()\n",
    "\n",
    "# 3. Calculate ROUGE scores\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = [scorer.score(t, p) for t, p in zip(truth, predicted)]\n",
    "\n",
    "# Aggregate ROUGE scores\n",
    "rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "\n",
    "# Print BLEU, BERTScore, and ROUGE scores\n",
    "print(f\"Mean BLEU Score: {average_bleu}\")\n",
    "print(f\"Mean Precision (BERTScore): {mean_precision}\")\n",
    "print(f\"Mean Recall (BERTScore): {mean_recall}\")\n",
    "print(f\"Mean F1 (BERTScore): {mean_f1}\")\n",
    "\n",
    "print(f\"Mean ROUGE-1 Score: {rouge1}\")\n",
    "print(f\"Mean ROUGE-2 Score: {rouge2}\")\n",
    "print(f\"Mean ROUGE-L Score: {rougeL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4579aa94dc2d4d27b72e62e6462b7373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points. The video starts off by showing different vehicles moving around a parking lot. In one frame, we can see a couple of cars driving around while some other people are walking around the area. In the next frame, we see a couple more cars and a bus driving around. This goes on for a few more frames, as we observe more cars and a truck passing through the parking lot. The video captures the busy activity of people and vehicles in the parking lot, providing a snapshot of daily life in the area. The video effectively showcases the dynamic nature of urban life, with people and vehicles constantly moving around the city. The different frames are a great representation of how life changes throughout the day and how people move through various locations within a city.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points. The video captures a farm with buildings and fields, with several farm buildings, such as barns and homes. The barns are seen from both the ground and from above. The crops grown in the farm field vary in color, from green to yellow. The trees around the farm buildings also vary in color, from green to red. The farm landscape provides an extensive view of the farm, showcasing the different farm activities that take place within the farm.\n",
      "\n",
      " Give a detailed account of the satellite video, with each frame depicting the same location at distinct points in time.\n",
      "In this satellite video, the camera pans out over the large building, showcasing the layout and scale of the structure from the very beginning. Each frame shows the same location at various points in time, providing insight into the progression of the building. As the camera continues to move forward, more details become visible. This could include changes in the architecture, alterations to the landscaping, or the addition of new structures such as offices, stores, or residential units. The process of analyzing the footage could provide valuable information about urban development, planning, or property values within that particular location. Overall, the video captures the development and progression of the area throughout different timeframes.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points. The video consists of several pictures of a large building and planes parked in the lot and on the runway. The first photo captures a plane sitting on the tarmac, while the others display the same building but at different times. The planes can be seen in various positions on the runway and lot, some parked and others in motion. Additionally, the photographs display a single airplane flying overhead, providing a comprehensive overview of the airport's operation.\n",
      "\n",
      " Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.\n",
      "In the satellite video, the same location is captured from different timestamps, which help in visualizing the movement of people and vehicles over time. This method, known as the time-lapse technique, provides a valuable insight into the area's activities, allowing us to observe changes, patterns, and trends in the area over a specific period.\n",
      "\n",
      "In this particular image, the time-lapse series showcases an airport tarmac, where planes can be seen, and people are moving around. The time-lapse video starts with an empty tarmac and gradually captures more planes arriving and taking off. People are also seen in the video, likely staff members attending to tasks and operations on the tarmac.\n",
      "\n",
      "By observing the changes and the movements in the time-lapse video, we can gain a comprehensive understanding of the airport's daily operations, the planes' schedules, and the people's activities on the tarmac.\n",
      "\n",
      " Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times. This video captures various scenes of airport operations, including the movement of planes and vehicles, as well as building construction or renovation projects.\n",
      "\n",
      " Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.\n",
      "\n",
      "The video begins by showing a large grassy field next to a big street. The camera then pans to the right, providing an aerial view of several car lots, and a huge building in the process of being built, possibly a warehouse. As the camera turns, the audience can see the construction work taking place behind the large building, as well as the numerous buildings and car lots that have already been completed. In addition, the viewer can observe several cars, possibly some of which belong to people working or visiting the area. The camera continues to pan to the right, showcasing various scenes related to the car lots and the ongoing construction. The video also includes views of a road where more cars and possibly other construction equipment can be observed.\n",
      "\n",
      "Throughout the video, the camera pans back and forth between the grassy field and the buildings, allowing the viewer to witness the vastness of the area as well as the progress made in constructing the buildings and car lots. Overall, the video captures the essence of the surrounding community, including the buildings, the field, the cars, and the ongoing construction work, providing a comprehensive and detailed visual representation of the area.\n",
      "\n",
      " Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    \"LanguageBind/Video-LLaVA-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for test in train_data:\n",
    "\n",
    "    true_value = test['conversations'][1]['value']\n",
    "\n",
    "    # Generate the predicted response\n",
    "    inputs = processor(text=test['conversations'][0]['value'], videos=read_video_pyav(f'VLM/Videos/{test[\"video\"]}', 0, 1e+10), padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generate_kwargs = {\"max_new_tokens\": 256, \"do_sample\": True, \"top_p\": 0.9}\n",
    "\n",
    "    output = model.generate(**inputs, **generate_kwargs)\n",
    "    generated_text = processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "    print(generated_text[0])\n",
    "    result_entry = {\n",
    "                'id': test['id'],\n",
    "                'video': test['video'],\n",
    "                'true': true_value,\n",
    "                'generated': generated_text[0]  # Add the cleaned text from the batch\n",
    "            }\n",
    "    results.append(result_entry)\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "with open('results_base.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c82f1b61fb247de868cab7f44d47c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image/video tokens in LLaVa-NeXT-Video should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n",
      "\n",
      " Give a detailed account of the satellite video, with each frame depicting the same location at distinct points in time.\n",
      "As we look at the satellite video, we can observe a scene of an overcast day with no distinct cloud formations visible. The trees in the area are not densely packed but are spread out across the landscape. The landscape appears to be a mixture of urban and rural areas, as we can see both structures and buildings. The colors in the video are not vivid, suggesting that the satellite does not have a high-definition camera. The frame captures a particular location and we can see the same view from different periods of time due to the video's repetitive nature, showing this area from different angles over time. The image does not show any movement, which is common in satellite videos, and the buildings and structures remain stationary. The video showcases the area from different angles, with no apparent change in the position of objects or people, indicating a static display.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n",
      "\n",
      " Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.\n",
      "\n",
      "The satellite video you're referring to could be an imagery sequence that captures a specific location over time, allowing us to observe changes that occur at that location. This type of imagery can be incredibly useful for studying urban development, natural disasters, or any other phenomenon that changes the landscape over time. The first frame might show the area as it looked in the past, while the subsequent frames may show its current state. One key point to note is that, even though the location appears the same across all frames, the information and changes captured over time can be quite different. This kind of analysis can help in tracking progress and development, or in understanding how a particular place has evolved. Imagine a cityscape, where a certain area might be captured over several years, showing the growth of new buildings, changes in vegetation, and even changes in weather patterns. The use of different colors or bands can denote different areas, and the images can be used for historical comparison or to illustrate trends over time.\n",
      "\n",
      "\n",
      "This kind of data visualization is especially helpful for urban planners who are interested in studying how cities change over time. By looking at the past and present state of an urban area, they can understand\n",
      "\n",
      " Elaborate on the changes in the location as seen in the satellite video, where each frame is a snapshot of the same place at different times.\n",
      "From the satellite video, we can observe a series of changes that indicate the dynamic nature of the city's landscape over time. One noticeable change is the presence of air traffic above, which may indicate the presence of an airport in the area at some point. Additionally, we can observe the alterations to the land use and urban planning of the city, including the construction of buildings and roads. As we progress through the timeline, we may also see the changes in the city's infrastructure, such as the addition of public transportation systems or the expansion of existing ones. We may also see the evolution of the city's street pattern and the appearance of different architectural styles, reflecting the city's history and development over the years. These snapshots capture the city's growth and change over a period of time, offering a comprehensive perspective on its transformation.\n",
      "\n",
      " Provide a detailed description of the satellite video, where each frame corresponds to a different time but the same location.\n",
      "\n",
      "\n",
      "The video depicts an area that appears to be a parking lot situated between some buildings, and there are several cars and trucks parked in this area. The majority of the cars are parked along a white line, and some are parked behind the buildings. The vehicles are mostly stationary, and no visible movement is apparent on them, indicating that they might be parked for a short duration. The background consists of buildings, possibly office spaces or residential, and some greenery, suggesting the parking lot is adjacent to a parking structure. The overall scene seems to be static with no significant activity occurring.\n",
      "\n",
      " Offer an elaborate explanation of the satellite video, where every frame captures the same location but at different times.\n",
      "\n",
      " Provide a report describing the satellite video, where each frame shows the same location at different time points.\n",
      "\n",
      "The video begins with a view from a satellite of a location on the Earth's surface. In this shot, we can see several cars and people moving about on the roads and sidewalks, giving the impression of a busy day in the city. As the video progresses, more and more frames are added to the display, showing the same location at different times, perhaps to demonstrate the changes in activity or traffic in the area. This could allow us to see the flow of people and vehicles at different points in the day, the changes in congestion, or even the impact of weather on the location. It would give us a detailed overview of how this area changes over time and would be useful for urban planners, researchers, or even for someone who is just curious about the dynamics of this particular area.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for test in train_data:\n",
    "\n",
    "    true_value = test['conversations'][1]['value']\n",
    "\n",
    "    # Generate the predicted response\n",
    "    inputs = processor(text=test['conversations'][0]['value'], videos=read_video_pyav(f'VLM/Videos/{test[\"video\"]}', 0, 1e+10), padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generate_kwargs = {\"max_new_tokens\": 128, \"do_sample\": True, \"top_p\": 0.9}\n",
    "\n",
    "    output = model.generate(**inputs, **generate_kwargs)\n",
    "    generated_text = processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "    print(generated_text[0])\n",
    "    result_entry = {\n",
    "                'id': test['id'],\n",
    "                'video': test['video'],\n",
    "                'true': true_value,\n",
    "                'generated': generated_text[0]  # Add the cleaned text from the batch\n",
    "            }\n",
    "    results.append(result_entry)\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "with open('results_base_next.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[1]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "results = []\n",
    "batch_size = 1  # Define your batch size here\n",
    "\n",
    "# processor.tokenizer.padding_side = \"left\" # during training, one always uses padding on the right\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Split test data into batches\n",
    "    for i in range(math.ceil(len(test_data) / batch_size)):\n",
    "        # Define the start and end index for the current batch\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(test_data))\n",
    "        \n",
    "        # Collect all texts and videos for the current batch\n",
    "        # batch_texts = [test['conversations'][0]['value'] for test in test_data[start_idx:end_idx]]\n",
    "        batch_texts = [f\"{test['conversations'][0]['value']}\" for test in test_data[start_idx:end_idx]]\n",
    "        batch_videos = [read_video_pyav(f'VLM/Videos/{test[\"video\"]}', 0, 1e+10) for test in test_data[start_idx:end_idx]]\n",
    "        \n",
    "        print(batch_texts)\n",
    "\n",
    "        # Process the entire batch at once\n",
    "        inputs = processor(text=batch_texts, videos=batch_videos, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        # print(inputs)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=False)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        print(generated_texts)\n",
    "        \n",
    "        cleaned_texts = [\n",
    "            generated_text for generated_text in generated_texts\n",
    "        ]\n",
    "        # Create results for the entire batch\n",
    "        for idx, test in enumerate(test_data[start_idx:end_idx]):\n",
    "            # print(cleaned_texts[idx])\n",
    "            true_value = test['conversations'][1]['value']\n",
    "            result_entry = {\n",
    "                'id': test['id'],\n",
    "                'video': test['video'],\n",
    "                'true': true_value,\n",
    "                'generated': cleaned_texts[idx]  # Add the cleaned text from the batch\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
